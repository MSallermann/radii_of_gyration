import polars as pl
from pathlib import Path
import itertools
import json

configfile: "config.yml"

######################################################################
# The database file from which the prot_data dicts are read in 
######################################################################

database_file = Path(config["database_file"])
if database_file.suffix == ".parquet":
    DATA_BASE = (
        pl.scan_parquet(config["database_file"]).select("key", "prot_data").collect()
    )
else:
    DATA_BASE = (
        pl.scan_ipc(config["database_file"]).select("key", "prot_data").collect()
    )

######################################################################
# Parse some information from the config.yml.file
######################################################################

# The command to invoke lammps. May be as simple as `lmp`, but can also be more complicated e.g. `srun --ntasks {resources.ntasks} path/to/lmp`
LAMMPS_BIN = config["lammps_binary"]

# The jinja template file to be used for the lammps script
TEMPLATE_FILE = config["template_file"]

# Number of simulation steps in the final nvt run
N_STEPS = config["n_steps"]
TIMESTEP = config["timestep"]
RG_SKIP = config["rg_skip"]
BOX_BUFFER = config["box_buffer"]

# Sequence globular domain criterion
MINIMUM_DOMAIN_LENGTH = config.get("minimum_domain_length", 0)
MINIMUM_IDR_LENGTH = config.get("minimum_idr_length", 0)

# Geometric globular domain criterion (these are all optional)
MIN_PAE_CUTOFF = config.get("min_pae_cutoff")
MEAN_PAE_CUTOFF = config.get("mean_pae_cutoff")
MIN_DISTANCE_CUTOFF = config.get("min_distance_cutoff")
MAX_COORDINATION_CUTOFF = config.get("max_coordination_cutoff")
COORDINATION_DISTANCE_CUTOFF = config.get("coordination_distance_cutoff")

# Globular domain interactions
GLOB_GLOB_SCALING = config["glob_glob_scaling"]
IDR_GLOB_SCALING = config["idr_glob_scaling"]

# Residue location
RESIDUE_LOCATION = config.get("residue_location", "Ca")

######################################################################
# Read in the samples file
######################################################################

SAMPLES_PATH = config.get("samples_file", "samples.json")

with open("./samples.json", "rb") as f:
    SAMPLES = json.load(f)

SAMPLES_START_IDX = int(config.get("samples_start_idx", 0))
SAMPLES_END_IDX = int(config.get("samples_end_idx", len(SAMPLES)))

print(f"{SAMPLES_START_IDX = }")
print(f"{SAMPLES_END_IDX = }")

drop_idx = []
for idx, k in enumerate(SAMPLES):
    if idx >= SAMPLES_START_IDX and idx<SAMPLES_END_IDX:
        continue
    else:
        drop_idx.append(k)

[SAMPLES.pop(idx) for idx in drop_idx]

######################################################################
# Define some utility functions
######################################################################

def get_db_row(wc):
    key = SAMPLES[wc.sample]["db_key"]
    db_filtered = DATA_BASE.filter(pl.col("key") == key)

    if len(db_filtered) != 1:
        msg = f"The key '{key}' has {len(db_filtered)} entries in the database. For correct operation, it needs to be *exactly* one!"
        raise Exception(msg)

    return db_filtered.row(0, named=True)


def get_prot_data(wc):
    return get_db_row(wc).get("prot_data")


# sample wild card to temperature
def get_temp(wc):
    return SAMPLES[wc.sample]["temperature"]


# sample wild card to ionic strength
def get_ionic_strength(wc):
    return SAMPLES[wc.sample]["ionic_strength"]


# sample wild card to threshold
def get_threshold(wc):
    return SAMPLES[wc.sample]["threshold"]


def get_start_idx(wc):
    return SAMPLES[wc.sample].get("start_idx")


def get_end_idx(wc):
    return SAMPLES[wc.sample].get("end_idx")


rule all:
    input:
        [f"results/aggregated_rg.csv" for sample in SAMPLES.keys()],


rule prepare_coarse_grained_inputs:
    localrule: True
    input:
        template_file=ancient(Path(TEMPLATE_FILE)),
    shadow:
        "copy-minimal"
    threads: 1
    output:
        directory("results/lammps_files/{sample}"),
    params:
        temp=get_temp,
        ionic_strength=get_ionic_strength,
        n_steps=N_STEPS,
        timestep=TIMESTEP,
        threshold=get_threshold,
        minimum_domain_length=MINIMUM_DOMAIN_LENGTH,
        minimum_idr_length=MINIMUM_IDR_LENGTH,
        prot_data=get_prot_data,
        min_pae_cutoff=MIN_PAE_CUTOFF,
        mean_pae_cutoff=MEAN_PAE_CUTOFF,
        min_distance_cutoff=MIN_DISTANCE_CUTOFF,
        max_coordination_cutoff=MAX_COORDINATION_CUTOFF,
        coordination_distance_cutoff=COORDINATION_DISTANCE_CUTOFF,
        box_buffer=BOX_BUFFER,
        start_idx=get_start_idx,
        end_idx=get_end_idx,
        glob_glob_scaling=GLOB_GLOB_SCALING,
        idr_glob_scaling=IDR_GLOB_SCALING,
        residue_location=RESIDUE_LOCATION
    script:
        workflow.source_path("scripts/coarse_grain_with_glob_scaling.py")


def get_n_threads(wildcards):
    start_idx = get_start_idx(wildcards)
    end_idx = get_end_idx(wildcards)

    if start_idx is not None and end_idx is not None:
        n_res = end_idx - start_idx
    elif (prot_data := get_prot_data(wildcards)) is not None:
        n_res = len(prot_data["sequence_one_letter"])
    else:
        return 1

    n_threads = n_res // 300 + 1
    return int(min(max(2, n_threads), 6))


rule run_lammps:
    input:
        "results/lammps_files/{sample}",
    output:
        folder=directory("results/lammps_runs/{sample}"),
        gyration="results/lammps_runs/{sample}/gyration.out",
    threads: get_n_threads
    resources:
        nodes=1,
        ntasks_per_node=get_n_threads,
        cpus_per_task=1,
        slurm_out=lambda wc: f"results/lammps_files/{wc.sample}/slurm.out",
        slurm_err=lambda wc: f"results/lammps_files/{wc.sample}/slurm.err",
    shell:
        "cp -r {input}/* {output.folder}\n"
        "cd {output.folder}\n"
        "export OMP_NUM_THREADS={resources.cpus_per_task}\n"
        f"{LAMMPS_BIN} -in script.lmp 1> /dev/null 2> /dev/null\n"


rule average_radius_of_gyration:
    localrule: True
    input:
        "results/lammps_runs/{sample}/gyration.out",
    output:
        rg_json="results/rg/{sample}/rg.json",
    params:
        n_skip=int(RG_SKIP),
    script:
        workflow.source_path("scripts/average_radius_of_gyration.py")


### This last rule simply tries to collect all rg values
KEYS = set()
for samp in SAMPLES.values():
    KEYS.update(samp.keys())

added_columns = {key: [sample.get(key) for sample in SAMPLES.values()] for key in KEYS}


rule aggregate_rg:
    localrule: True
    threads: 1
    input:
        [f"results/rg/{sample}/rg.json" for sample in SAMPLES.keys()],
    params:
        add_columns={**added_columns},
        ignore_columns=["file"],
    output:
        "results/aggregated_rg.csv",
    script:
        workflow.source_path("scripts/aggregate_json.py")
