import polars as pl
from pathlib import Path
import itertools
import json
from dataclasses import dataclass, asdict

configfile: "config.yml"

######################################################################
# Parse some information from the config.yml.file
######################################################################

@dataclass
class MyConfig:
    database_file : str
    samples_file : str

    lammps_binary : str | None
    lammps_script_template : str | None

    n_steps : int
    timestep : float
    rg_skip : int
    box_buffer : float
    n_save_rg : int
    n_save_traj : int

    samples_start_idx : int = 0
    samples_end_idx : int | None = None

    minimum_domain_length : int = 0
    minimum_idr_length : int = 0

    min_pae_cutoff : float | None = None
    mean_pae_cutoff : float | None = None
    min_distance_cutoff : float | None = None
    max_coordination_cutoff : int | None = None
    coordination_distance_cutoff : float | None = None

    glob_glob_scaling : float | None = None
    idr_glob_scaling : float | None = None

    residue_location : str | None = "Ca"
    calvados_residues_file : str | None = None

myconfig = MyConfig(**config)

######################################################################
# The database file from which the prot_data dicts are read in 
######################################################################

database_file = Path(myconfig.database_file)
if database_file.suffix == ".parquet":
    DATA_BASE = (
        pl.scan_parquet(database_file).select("key", "prot_data").collect()
    )
else:
    DATA_BASE = (
        pl.scan_ipc(database_file).select("key", "prot_data").collect()
    )


######################################################################
# Read in the samples file
######################################################################

with open(myconfig.samples_file, "rb") as f:
    SAMPLES = json.load(f)

SAMPLES_START_IDX = myconfig.samples_start_idx
SAMPLES_END_IDX = myconfig.samples_end_idx if myconfig.samples_end_idx is not None else len(SAMPLES)

print(f"{SAMPLES_START_IDX = }")
print(f"{SAMPLES_END_IDX = }")

drop_idx = []
for idx, k in enumerate(SAMPLES):
    if idx >= SAMPLES_START_IDX and idx<SAMPLES_END_IDX:
        continue
    else:
        drop_idx.append(k)

[SAMPLES.pop(idx) for idx in drop_idx]

######################################################################
# Define some utility functions
######################################################################

def get_db_row(wc):
    key = SAMPLES[wc.sample]["db_key"]
    db_filtered = DATA_BASE.filter(pl.col("key") == key)

    if len(db_filtered) != 1:
        msg = f"The key '{key}' has {len(db_filtered)} entries in the database. For correct operation, it needs to be *exactly* one!"
        raise Exception(msg)

    return db_filtered.row(0, named=True)


def get_prot_data(wc):
    return get_db_row(wc).get("prot_data")


# sample wild card to temperature
def get_temp(wc):
    return SAMPLES[wc.sample]["temperature"]

# sample wild card to ionic strength
def get_ionic_strength(wc):
    return SAMPLES[wc.sample]["ionic_strength"]


# sample wild card to threshold
def get_threshold(wc):
    return SAMPLES[wc.sample]["threshold"]


def get_start_idx(wc):
    return SAMPLES[wc.sample].get("start_idx")

def get_end_idx(wc):
    return SAMPLES[wc.sample].get("end_idx")


######################################################################
# Target rules
######################################################################

rule mpipi_prepare:
    input:
        [f"results/lammps_files/{sample}" for sample in SAMPLES]

rule mpipi_avg:
    input:
        [f"results/rg/{sample}/rg.json" for sample in SAMPLES]

rule mpipi_aggregate:
    input:
        "results/aggregated_rg.csv"

rule mpipi:
    input:
        rules.mpipi_prepare.input, rules.mpipi_avg.input, rules.mpipi_aggregate.input

rule calvados:
    input:
        "results/aggregated_rg_calvados.csv"

######################################################################
# Other rules
######################################################################

rule prepare_coarse_grained_inputs:
    localrule: True
    input:
        template_file=ancient(myconfig.lammps_script_template),
    shadow:
        "copy-minimal"
    threads: 1
    output:
        directory("results/lammps_files/{sample}"),
    params:
        temp=get_temp,
        ionic_strength=get_ionic_strength,
        prot_data=get_prot_data,
        threshold=get_threshold,
        start_idx=get_start_idx,
        end_idx=get_end_idx,
        n_steps=myconfig.n_steps,
        timestep=myconfig.timestep,
        minimum_domain_length=myconfig.minimum_domain_length,
        minimum_idr_length=myconfig.minimum_idr_length,
        min_pae_cutoff=myconfig.min_pae_cutoff,
        mean_pae_cutoff=myconfig.mean_pae_cutoff,
        min_distance_cutoff=myconfig.min_distance_cutoff,
        max_coordination_cutoff=myconfig.max_coordination_cutoff,
        coordination_distance_cutoff=myconfig.coordination_distance_cutoff,
        box_buffer=myconfig.box_buffer,
        glob_glob_scaling=myconfig.glob_glob_scaling,
        idr_glob_scaling=myconfig.idr_glob_scaling,
        residue_location=myconfig.residue_location,
        n_save_rg=myconfig.n_save_rg,
        n_save_traj=myconfig.n_save_traj
    script:
        workflow.source_path("scripts/coarse_grain_with_glob_scaling.py")


def get_n_threads(wildcards):
    start_idx = get_start_idx(wildcards)
    end_idx = get_end_idx(wildcards)

    if start_idx is not None and end_idx is not None:
        n_res = end_idx - start_idx
    elif (prot_data := get_prot_data(wildcards)) is not None:
        n_res = len(prot_data["sequence_one_letter"])
    else:
        return 1

    n_threads = n_res // 300 + 1
    return int(min(max(2, n_threads), 6))


rule run_lammps:
    input:
        "results/lammps_files/{sample}",
    output:
        folder=directory("results/lammps_runs/{sample}"),
        gyration="results/lammps_runs/{sample}/gyration.out",
    threads: get_n_threads
    resources:
        nodes=1,
        ntasks_per_node=get_n_threads,
        cpus_per_task=1,
        slurm_out=lambda wc: f"results/lammps_files/{wc.sample}/slurm.out",
        slurm_err=lambda wc: f"results/lammps_files/{wc.sample}/slurm.err",
    shell:
        "cp -r {input}/* {output.folder}\n"
        "cd {output.folder}\n"
        "export OMP_NUM_THREADS={resources.cpus_per_task}\n"
        f"{myconfig.lammps_binary} -in script.lmp 1> /dev/null 2> /dev/null\n"


rule average_radius_of_gyration:
    localrule: True
    input:
        "results/lammps_runs/{sample}/gyration.out",
    output:
        rg_json="results/rg/{sample}/rg.json",
    params:
        n_skip=int(myconfig.rg_skip),
    script:
        workflow.source_path("scripts/average_radius_of_gyration.py")

### This last rule simply tries to collect all rg values
KEYS = set()
for samp in SAMPLES.values():
    KEYS.update(samp.keys())

added_columns = {key: [sample.get(key) for sample in SAMPLES.values()] for key in KEYS}


rule aggregate_rg:
    localrule: True
    threads: 1
    input:
        [f"results/rg/{sample}/rg.json" for sample in SAMPLES.keys()],
    params:
        add_columns={**added_columns},
        ignore_columns=["file"],
    output:
        "results/aggregated_rg.csv",
    script:
        workflow.source_path("scripts/aggregate_json.py")


rule prepare_calvados:
    localrule: True
    threads: 1
    output:
        directory("results/calvados_files/{sample}"),
    params:
        name="{sample}",
        temp=get_temp,
        ionic_strength=get_ionic_strength,
        n_steps=myconfig.n_steps,
        prot_data=get_prot_data,
        box_buffer=myconfig.box_buffer,
        start_idx=get_start_idx,
        end_idx=get_end_idx,
        n_save=myconfig.n_save_rg,
        residues_file=myconfig.calvados_residues_file,
        rg_skip=myconfig.rg_skip
    script:
        workflow.source_path("scripts/prepare_calvados_run.py")

rule run_calvados:
    threads: 1
    input: "results/calvados_files/{sample}"
    output:
        rgs="results/calvados_runs/{sample}/rg.json",
        folder=directory("results/calvados_runs/{sample}"),
    shell:
        "cp -r {input}/* {output.folder}\n"
        "cd {output.folder}\n"
        "python3 run.py --path ."


rule aggregate_rg_calvados:
    localrule: True
    threads: 1
    input:
        [f"results/calvados_runs/{sample}/rg.json" for sample in SAMPLES.keys()],
    params:
        add_columns={**added_columns},
        ignore_columns=["file"],
    output:
        "results/aggregated_rg_calvados.csv",
    script:
        workflow.source_path("scripts/aggregate_json.py")

